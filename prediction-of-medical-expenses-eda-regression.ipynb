{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12380505,"sourceType":"datasetVersion","datasetId":7806507}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":2155.624375,"end_time":"2025-04-13T21:19:25.629501","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-04-13T20:43:30.005126","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <link href=\"https://fonts.googleapis.com/css2?family=Poppins:wght@700&display=swap\" rel=\"stylesheet\">\n</head>\n<body>\n    <center>\n        <h2 style=\"font-weight: bolder; color: #488286; font-size: 180%; font-family: 'Times New Roman', sans-serif; text-shadow: 2px 2px 5px rgba(0, 0, 0, 0.1);\">\n            Regression-Driven Prediction of Medical Insurance Expenses & Premiums\n        </h2>\n    </center>\n</body>\n</html>","metadata":{"papermill":{"duration":0.004927,"end_time":"2025-04-13T20:43:34.003459","exception":false,"start_time":"2025-04-13T20:43:33.998532","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"font-family: 'Times New Roman', serif; font-size: 20px; color: #000000; text-align: center; padding: 15px; border: 2px solid #ffffff; display: block; margin: auto;\">\n    Melissa Jalali Monfared\n</div>","metadata":{"papermill":{"duration":0.003717,"end_time":"2025-04-13T20:43:34.011243","exception":false,"start_time":"2025-04-13T20:43:34.007526","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id='top'></a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n <p style=\"font-family:newtimeroman;color:#000000;font-size:120%;text-align:center;border-radius:40px 40px;\">TABLE OF CONTENTS</p>   \n    \n* **[- | Introduction & Data Preprocessing](#0)**\n    \n* **[- | Data Visualization](#1)**\n    \n* **[- | Modeling](#2)**","metadata":{"papermill":{"duration":0.003717,"end_time":"2025-04-13T20:43:34.018783","exception":false,"start_time":"2025-04-13T20:43:34.015066","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<img src=\"https://www.gannett-cdn.com/authoring/authoring-images/2025/06/12/USAT/84166421007-usatgraphics-healthcarecoststopper.png?crop=3388,1906,x119,y5&width=2560\" width=\"800\" height=\"600\">","metadata":{"papermill":{"duration":0.003722,"end_time":"2025-04-13T20:43:34.026277","exception":false,"start_time":"2025-04-13T20:43:34.022555","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<a id=\"0\"></a>\n\n<h1 style=\"\n    background-color:#488286; \n    background-size: cover;\n    background-position: center;\n    font-family: 'Times New Roman', serif;\n    font-size: 1.5em;\n    color: white;\n    text-align: center;\n    padding: 15px;\n    border-radius: 15px 50px;\n    border: 1px solid black;\n\">\n    Introduction & Data Preprocessing\n</h1>","metadata":{"papermill":{"duration":0.003688,"end_time":"2025-04-13T20:43:34.033918","exception":false,"start_time":"2025-04-13T20:43:34.03023","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<div style=\"border: 2px solid #305252; padding: 15px; background-color: #ffffff;\">\n\n### <span style=\"color: #305252;\">Medical Insurance Expenses & Premium Analysis</span>\n\nUnderstanding the financial dynamics of healthcare is increasingly critical in designing fair and efficient insurance systems. This project presents a comprehensive data-driven analysis of medical insurance expenses and premiums, with a focus on uncovering the demographic and behavioral factors that influence healthcare costs.\n\nThe dataset used in this study includes detailed records of policyholders, capturing both demographic attributes (such as age, gender, BMI, number of children) and financial metrics (medical expenses and insurance premiums). By leveraging statistical modeling and machine learning techniques, I aim to build predictive models that estimate medical costs and premiums with high accuracy.\n\nThis exploration not only contributes to optimizing pricing strategies for insurance providers but also sheds light on equity in access and cost across different population segments. The project reflects my commitment to applying data science for impactful insights in the healthcare domain.\n\n### <span style=\"color: #305252;\">About Dataset</span>\n\n| Column                 | Description                                                                 |\n|------------------------|-----------------------------------------------------------------------------|\n| **age**                | Age of the policyholder.                                                    |\n| **gender**             | Gender of the policyholder (male/female).                                   |\n| **bmi**                | Body Mass Index of the policyholder.                                        |\n| **children**           | Number of children covered by the insurance policy.                         |\n| **discount_eligibility** | Indicates whether the policyholder is eligible for a discount (yes/no).     |\n| **region**             | Geographic region where the policyholder resides (e.g., southeast, northwest). |\n| **expenses**           | Actual medical costs incurred by the policyholder. *(Target variable 1)*     |\n| **premium**            | Insurance premium charged to the policyholder. *(Target variable 2)*         |","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\n\nfrom ydata_profiling import ProfileReport\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom matplotlib.colors import LinearSegmentedColormap\n\nfrom sklearn.metrics import (\n    accuracy_score, roc_auc_score, confusion_matrix, classification_report,\n    mean_absolute_error, mean_squared_error, r2_score\n)\n\nfrom sklearn.model_selection import (\n    train_test_split, cross_validate, GridSearchCV,\n    KFold, cross_val_score\n)\n\npd.set_option('display.max_columns', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\ncustom_palette = [\"#B7D5D4\", \"#77878B\", \"#488286\", \"#305252\"]\nsns.set_palette(sns.color_palette(custom_palette))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:32:06.420817Z","iopub.execute_input":"2025-07-05T18:32:06.421159Z","iopub.status.idle":"2025-07-05T18:32:19.146026Z","shell.execute_reply.started":"2025-07-05T18:32:06.421132Z","shell.execute_reply":"2025-07-05T18:32:19.145028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/health-insurance-dataset/medical_insurance.csv')\ndata = pd.DataFrame(data)\ndata","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:15.938818Z","iopub.execute_input":"2025-07-05T18:39:15.939184Z","iopub.status.idle":"2025-07-05T18:39:15.967249Z","shell.execute_reply.started":"2025-07-05T18:39:15.939156Z","shell.execute_reply":"2025-07-05T18:39:15.966456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:18.412372Z","iopub.execute_input":"2025-07-05T18:39:18.412949Z","iopub.status.idle":"2025-07-05T18:39:18.418334Z","shell.execute_reply.started":"2025-07-05T18:39:18.412925Z","shell.execute_reply":"2025-07-05T18:39:18.41759Z"},"papermill":{"duration":44.891171,"end_time":"2025-04-13T20:45:08.620905","exception":false,"start_time":"2025-04-13T20:44:23.729734","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:18.527903Z","iopub.execute_input":"2025-07-05T18:39:18.52854Z","iopub.status.idle":"2025-07-05T18:39:18.533842Z","shell.execute_reply.started":"2025-07-05T18:39:18.528512Z","shell.execute_reply":"2025-07-05T18:39:18.533061Z"},"papermill":{"duration":3.866148,"end_time":"2025-04-13T20:45:12.496648","exception":false,"start_time":"2025-04-13T20:45:08.6305","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- There are 8 columns & 1338 rows in this dataset.","metadata":{"execution":{"iopub.execute_input":"2025-04-13T20:45:12.515945Z","iopub.status.busy":"2025-04-13T20:45:12.515712Z","iopub.status.idle":"2025-04-13T20:45:12.519859Z","shell.execute_reply":"2025-04-13T20:45:12.519088Z"},"papermill":{"duration":0.014996,"end_time":"2025-04-13T20:45:12.521031","exception":false,"start_time":"2025-04-13T20:45:12.506035","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def check_df(data: object, head: object = 5) -> object:\n    print(\"\\nShape\")\n    print(data.shape)\n    print(\"\\nTypes\")\n    print(data.dtypes)\n    print(\"\\nNANs\")\n    print(data.isnull().sum())\n    print(\"\\nInfo\")\n    print(data.info())\ncheck_df(data)","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:18.728694Z","iopub.execute_input":"2025-07-05T18:39:18.729078Z","iopub.status.idle":"2025-07-05T18:39:18.74607Z","shell.execute_reply.started":"2025-07-05T18:39:18.729039Z","shell.execute_reply":"2025-07-05T18:39:18.745066Z"},"papermill":{"duration":0.982717,"end_time":"2025-04-13T20:45:13.512719","exception":false,"start_time":"2025-04-13T20:45:12.530002","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print('Number of duplicated rows: ' , len(data[data.duplicated()]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:18.941374Z","iopub.execute_input":"2025-07-05T18:39:18.941681Z","iopub.status.idle":"2025-07-05T18:39:18.949532Z","shell.execute_reply.started":"2025-07-05T18:39:18.941659Z","shell.execute_reply":"2025-07-05T18:39:18.94864Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = data.drop_duplicates()\ndata = data.reset_index(drop=True)\nprint('Number of duplicated rows after cleaning:', data.duplicated().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:18.950906Z","iopub.execute_input":"2025-07-05T18:39:18.951231Z","iopub.status.idle":"2025-07-05T18:39:18.971094Z","shell.execute_reply.started":"2025-07-05T18:39:18.951201Z","shell.execute_reply":"2025-07-05T18:39:18.970158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from matplotlib.colors import LinearSegmentedColormap\ncustom_colors = [\"#B7D5D4\", \"#77878B\", \"#488286\", \"#305252\"]\ncustom_cmap = LinearSegmentedColormap.from_list(\"custom_bone\", custom_colors)\n\nplt.figure(figsize=(22, 4))\nsns.heatmap(\n    (data.isna().sum()).to_frame(name='').T,\n    cmap=custom_cmap,\n    annot=True,\n    fmt='0.0f'\n).set_title('Count of Missing Values', fontsize=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:19.572619Z","iopub.execute_input":"2025-07-05T18:39:19.572954Z","iopub.status.idle":"2025-07-05T18:39:19.823387Z","shell.execute_reply.started":"2025-07-05T18:39:19.57292Z","shell.execute_reply":"2025-07-05T18:39:19.822387Z"},"papermill":{"duration":0.514651,"end_time":"2025-04-13T20:45:14.038294","exception":false,"start_time":"2025-04-13T20:45:13.523643","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"styled = data.describe().T.style.background_gradient(cmap=custom_cmap, axis=1)\nstyled","metadata":{"papermill":{"duration":0.012517,"end_time":"2025-04-13T20:45:14.06387","exception":false,"start_time":"2025-04-13T20:45:14.051353","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:19.824786Z","iopub.execute_input":"2025-07-05T18:39:19.825066Z","iopub.status.idle":"2025-07-05T18:39:19.849126Z","shell.execute_reply.started":"2025-07-05T18:39:19.825044Z","shell.execute_reply":"2025-07-05T18:39:19.848113Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- we can see statistical information on the table above.","metadata":{"execution":{"iopub.execute_input":"2025-04-13T20:45:14.089484Z","iopub.status.busy":"2025-04-13T20:45:14.088993Z","iopub.status.idle":"2025-04-13T20:45:14.663737Z","shell.execute_reply":"2025-04-13T20:45:14.662831Z"},"papermill":{"duration":0.589321,"end_time":"2025-04-13T20:45:14.664999","exception":false,"start_time":"2025-04-13T20:45:14.075678","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Finding unique data\ndata.apply(lambda x: len(x.unique()))","metadata":{"papermill":{"duration":0.012122,"end_time":"2025-04-13T20:45:14.715225","exception":false,"start_time":"2025-04-13T20:45:14.703103","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:19.901439Z","iopub.execute_input":"2025-07-05T18:39:19.901775Z","iopub.status.idle":"2025-07-05T18:39:19.911146Z","shell.execute_reply.started":"2025-07-05T18:39:19.901749Z","shell.execute_reply":"2025-07-05T18:39:19.910171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unique = data.nunique().sort_values()\nunique_values = data.apply(lambda x: x.unique())\npd.DataFrame({'Number of Unique Values': unique, 'Unique Values': unique_values})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:20.035537Z","iopub.execute_input":"2025-07-05T18:39:20.036031Z","iopub.status.idle":"2025-07-05T18:39:20.054413Z","shell.execute_reply.started":"2025-07-05T18:39:20.035975Z","shell.execute_reply":"2025-07-05T18:39:20.05334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dropping non-numerical data\nhm = data.drop(columns=['discount_eligibility', 'gender', 'region'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:20.158876Z","iopub.execute_input":"2025-07-05T18:39:20.159251Z","iopub.status.idle":"2025-07-05T18:39:20.165317Z","shell.execute_reply.started":"2025-07-05T18:39:20.159226Z","shell.execute_reply":"2025-07-05T18:39:20.164089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate the profile report\nprofile = ProfileReport(data, \n                        title='Dataset Report', \n                        minimal=True, \n                        progress_bar=False, \n                        samples=None, \n                        correlations=None, \n                        interactions=None, \n                        explorative=True, \n                        notebook={'iframe': {'height': '600px'}}, \n                        html={'style': {'primary_color': '#C44536'}}, \n                        missing_diagrams={'heatmap': False, 'dendrogram': False})\n\n# Display the report as an iframe in the notebook\nprofile.to_notebook_iframe()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:22.684091Z","iopub.execute_input":"2025-07-05T18:39:22.684731Z","iopub.status.idle":"2025-07-05T18:39:27.180462Z","shell.execute_reply.started":"2025-07-05T18:39:22.684707Z","shell.execute_reply":"2025-07-05T18:39:27.17943Z"},"papermill":{"duration":0.02039,"end_time":"2025-04-13T20:45:14.747866","exception":false,"start_time":"2025-04-13T20:45:14.727476","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"1\"></a>\n\n<h1 style=\"\n    background-color:#488286; \n    background-size: cover;\n    background-position: center;\n    font-family: 'Times New Roman', serif;\n    font-size: 1.5em;\n    color: white;\n    text-align: center;\n    padding: 15px;\n    border-radius: 15px 50px;\n    border: 1px solid black;\n\">\n    Data Visualization\n</h1>","metadata":{}},{"cell_type":"code","source":"sns.set_palette(sns.color_palette(custom_palette))\ncolumns = ['gender', 'children', 'discount_eligibility', 'region']\ntitles = ['Gender', 'Children', 'Discount Eligibility', 'Region']\n\ndef get_explode(values, threshold=10):\n    total = sum(values)\n    return [0.3 if (v / total) * 100 < threshold else 0 for v in values]\n\nfig, ax = plt.subplots(figsize=(15, 12), nrows=2, ncols=2)\n\nfor i, column in enumerate(columns):\n    row = i // 2\n    col = i % 2\n    values = data[column].value_counts()\n    explode = get_explode(values)\n    values.plot.pie(\n        autopct='%1.1f%%',\n        ax=ax[row, col],\n        startangle=90,\n        textprops={'color': 'black', 'fontsize': 10},\n        pctdistance=0.7,\n        explode=explode,\n        colors=custom_palette\n    )\n    ax[row, col].set_title(titles[i], fontsize=14, loc='left')\n    ax[row, col].set_ylabel('')\n\nplt.tight_layout(pad=2.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:32.641796Z","iopub.execute_input":"2025-07-05T18:39:32.64218Z","iopub.status.idle":"2025-07-05T18:39:33.118045Z","shell.execute_reply.started":"2025-07-05T18:39:32.642154Z","shell.execute_reply":"2025-07-05T18:39:33.117042Z"},"papermill":{"duration":0.094004,"end_time":"2025-04-13T20:45:14.854455","exception":false,"start_time":"2025-04-13T20:45:14.760451","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_palette = [\"#B7D5D4\", \"#77878B\", \"#488286\", \"#305252\"]\n\ncolumns = ['gender', 'children', 'discount_eligibility', 'region']\n\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\nfor i, col_name in enumerate(columns):\n    row = i // 2\n    col = i % 2\n    ax = sns.countplot(ax=axes[row, col], x=data[col_name], palette=custom_palette)\n    total = len(data[col_name])\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height() / total)\n        x = p.get_x() + p.get_width() / 2\n        y = p.get_height()\n        ax.annotate(percentage, (x, y), ha='center', va='bottom', fontsize=10)\n    axes[row, col].set_title(col_name.title(), fontsize=14)\n    axes[row, col].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:33.119589Z","iopub.execute_input":"2025-07-05T18:39:33.11999Z","iopub.status.idle":"2025-07-05T18:39:33.820835Z","shell.execute_reply.started":"2025-07-05T18:39:33.119968Z","shell.execute_reply":"2025-07-05T18:39:33.819603Z"},"papermill":{"duration":0.100496,"end_time":"2025-04-13T20:45:14.96778","exception":false,"start_time":"2025-04-13T20:45:14.867284","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"styled_corr = hm.corr(numeric_only=True).T.style.background_gradient(cmap=custom_cmap, axis=1)\nstyled_corr","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:37.736271Z","iopub.execute_input":"2025-07-05T18:39:37.736595Z","iopub.status.idle":"2025-07-05T18:39:37.752339Z","shell.execute_reply.started":"2025-07-05T18:39:37.736572Z","shell.execute_reply":"2025-07-05T18:39:37.751322Z"},"papermill":{"duration":2.718869,"end_time":"2025-04-13T20:45:17.772212","exception":false,"start_time":"2025-04-13T20:45:15.053343","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nsns.heatmap(\n    hm.corr(numeric_only=True),\n    cmap=custom_cmap,\n    annot=True,\n    linewidths=0.6,\n    cbar=False)\n\nplt.xticks(rotation=60, size=10)\nplt.yticks(size=10)\nplt.title('Analysis of Correlations', size=20)\nplt.tight_layout()\nplt.show()","metadata":{"papermill":{"duration":0.012522,"end_time":"2025-04-13T20:45:17.798013","exception":false,"start_time":"2025-04-13T20:45:17.785491","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:39:38.061303Z","iopub.execute_input":"2025-07-05T18:39:38.061629Z","iopub.status.idle":"2025-07-05T18:39:38.369692Z","shell.execute_reply.started":"2025-07-05T18:39:38.061607Z","shell.execute_reply":"2025-07-05T18:39:38.368916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"corr = hm.corr(numeric_only=True)\nf, ax = plt.subplots(figsize=(15, 5))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncut_off = 0.25\nextreme_1 = 0.5\nextreme_2 = 0.75\nextreme_3 = 0.9\nmask |= np.abs(corr) < cut_off\ncorr = corr[~mask]\nremove_empty_rows_and_cols = True\nif remove_empty_rows_and_cols:\n    wanted_cols = np.flatnonzero(np.count_nonzero(~mask, axis=1))\n    wanted_rows = np.flatnonzero(np.count_nonzero(~mask, axis=0))\n    corr = corr.iloc[wanted_cols, wanted_rows]\n\nannot = [[f\"{val:.4f}\"\n          + ('' if abs(val) < extreme_1 else '\\n*')\n          + ('' if abs(val) < extreme_2 else '*')\n          + ('' if abs(val) < extreme_3 else '*')\n          for val in row] for row in corr.to_numpy()]\nheatmap = sns.heatmap(corr, vmin=-1, vmax=1, annot=annot, fmt='', cmap=custom_cmap)\nheatmap.set_title('Triangle Correlation Heatmap', fontdict={'fontsize': 12}, pad=16)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:40.018984Z","iopub.execute_input":"2025-07-05T18:39:40.01933Z","iopub.status.idle":"2025-07-05T18:39:40.240249Z","shell.execute_reply.started":"2025-07-05T18:39:40.019309Z","shell.execute_reply":"2025-07-05T18:39:40.239271Z"},"papermill":{"duration":0.032072,"end_time":"2025-04-13T20:45:17.842575","exception":false,"start_time":"2025-04-13T20:45:17.810503","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure (figsize = (15 , 3) , dpi = 100)\nheatmap = sns.heatmap (hm.corr(numeric_only=True)[['age']].sort_values (by = 'age', ascending = False), vmin = -1, vmax = 1, annot = True, cmap = custom_cmap)\nheatmap.set_title ('Features Correlating with age', fontdict = {'fontsize':12} , pad = 18);","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:39:59.340315Z","iopub.execute_input":"2025-07-05T18:39:59.340699Z","iopub.status.idle":"2025-07-05T18:39:59.601355Z","shell.execute_reply.started":"2025-07-05T18:39:59.340671Z","shell.execute_reply":"2025-07-05T18:39:59.600417Z"},"papermill":{"duration":0.377102,"end_time":"2025-04-13T20:45:18.232823","exception":false,"start_time":"2025-04-13T20:45:17.855721","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure (figsize = (15 , 3) , dpi = 100)\nheatmap = sns.heatmap (hm.corr(numeric_only=True)[['premium']].sort_values (by = 'premium', ascending = False), vmin = -1, vmax = 1, annot = True, cmap = custom_cmap)\nheatmap.set_title ('Features Correlating with premium', fontdict = {'fontsize':12} , pad = 18);","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:40:01.941881Z","iopub.execute_input":"2025-07-05T18:40:01.942982Z","iopub.status.idle":"2025-07-05T18:40:02.18767Z","shell.execute_reply.started":"2025-07-05T18:40:01.942938Z","shell.execute_reply":"2025-07-05T18:40:02.186641Z"},"papermill":{"duration":1930.514764,"end_time":"2025-04-13T21:17:28.76725","exception":false,"start_time":"2025-04-13T20:45:18.252486","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure (figsize = (15 , 3) , dpi = 100)\nheatmap = sns.heatmap (hm.corr(numeric_only=True)[['expenses']].sort_values (by = 'expenses', ascending = False), vmin = -1, vmax = 1, annot = True, cmap = custom_cmap)\nheatmap.set_title ('Features Correlating with expenses', fontdict = {'fontsize':12} , pad = 18);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:06.63776Z","iopub.execute_input":"2025-07-05T18:40:06.63813Z","iopub.status.idle":"2025-07-05T18:40:06.893911Z","shell.execute_reply.started":"2025-07-05T18:40:06.638105Z","shell.execute_reply":"2025-07-05T18:40:06.892993Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure (figsize = (15 , 3) , dpi = 100)\nheatmap = sns.heatmap (hm.corr(numeric_only=True)[['bmi']].sort_values (by = 'bmi', ascending = False), vmin = -1, vmax = 1, annot = True, cmap = custom_cmap)\nheatmap.set_title ('Features Correlating with bmi', fontdict = {'fontsize':12} , pad = 18);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:06.895806Z","iopub.execute_input":"2025-07-05T18:40:06.896107Z","iopub.status.idle":"2025-07-05T18:40:07.149634Z","shell.execute_reply.started":"2025-07-05T18:40:06.896086Z","shell.execute_reply":"2025-07-05T18:40:07.1485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure (figsize = (15 , 3) , dpi = 100)\nheatmap = sns.heatmap (hm.corr(numeric_only=True)[['children']].sort_values (by = 'children', ascending = False), vmin = -1, vmax = 1, annot = True, cmap = custom_cmap)\nheatmap.set_title ('Features Correlating with children', fontdict = {'fontsize':12} , pad = 18);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:07.150706Z","iopub.execute_input":"2025-07-05T18:40:07.151094Z","iopub.status.idle":"2025-07-05T18:40:07.405691Z","shell.execute_reply.started":"2025-07-05T18:40:07.151069Z","shell.execute_reply":"2025-07-05T18:40:07.404686Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_palette2 = [\n    \"#B7D5D4\",\n    \"#77878B\",\n    \"#488286\",\n    \"#305252\",\n    \"#92B8A7\",\n    \"#3B5A54\"]\n\nhm['children'] = hm['children'].astype(str)\n\nsns.pairplot(\n    data=hm,\n    diag_kind='kde',\n    hue='children',\n    palette=custom_palette2,\n    corner=True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:40:07.406711Z","iopub.execute_input":"2025-07-05T18:40:07.406953Z","iopub.status.idle":"2025-07-05T18:40:11.546646Z","shell.execute_reply.started":"2025-07-05T18:40:07.406933Z","shell.execute_reply":"2025-07-05T18:40:11.545646Z"},"papermill":{"duration":1.188329,"end_time":"2025-04-13T21:17:32.061315","exception":false,"start_time":"2025-04-13T21:17:30.872986","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ff = ['age', 'children', 'expenses', 'premium']\n\ndf_melted = data.melt(id_vars=['bmi', 'gender'], value_vars=ff, var_name='variable', value_name='value')\n\ng = sns.relplot(\n    data=df_melted,\n    x='bmi',\n    y='value',\n    hue='gender',\n    col='variable',\n    kind='scatter',\n    palette=custom_palette2,\n    facet_kws={'sharey': False, 'sharex': True},\n    height=5,\n    aspect=1)\n\ng.set_titles(col_template=\"{col_name}\", size=16)\ng.set_axis_labels(\"BMI\", \"\")\nfor ax in g.axes.flatten():\n    ax.tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:11.548485Z","iopub.execute_input":"2025-07-05T18:40:11.548826Z","iopub.status.idle":"2025-07-05T18:40:13.304621Z","shell.execute_reply.started":"2025-07-05T18:40:11.548803Z","shell.execute_reply":"2025-07-05T18:40:13.303442Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ff = ['bmi', 'children', 'expenses', 'premium']\n\ndf_melted = data.melt(id_vars=['age', 'gender'], value_vars=ff, var_name='variable', value_name='value')\n\ng = sns.relplot(\n    data=df_melted,\n    x='age',\n    y='value',\n    hue='gender',\n    col='variable',\n    kind='scatter',\n    palette=custom_palette2,\n    facet_kws={'sharey': False, 'sharex': True},\n    height=5,\n    aspect=1)\n\ng.set_titles(col_template=\"{col_name}\", size=16)\ng.set_axis_labels(\"AGE\", \"\")\nfor ax in g.axes.flatten():\n    ax.tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:40:13.305742Z","iopub.execute_input":"2025-07-05T18:40:13.306067Z","iopub.status.idle":"2025-07-05T18:40:15.05437Z","shell.execute_reply.started":"2025-07-05T18:40:13.306032Z","shell.execute_reply":"2025-07-05T18:40:15.052866Z"},"papermill":{"duration":10.563106,"end_time":"2025-04-13T21:17:43.36345","exception":false,"start_time":"2025-04-13T21:17:32.800344","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ff = ['bmi', 'age', 'expenses', 'premium']\n\ndf_melted = data.melt(id_vars=['children', 'gender'], value_vars=ff, var_name='variable', value_name='value')\n\ng = sns.relplot(\n    data=df_melted,\n    x='children',\n    y='value',\n    hue='gender',\n    col='variable',\n    kind='scatter',\n    palette=custom_palette2,\n    facet_kws={'sharey': False, 'sharex': True},\n    height=5,\n    aspect=1)\n\ng.set_titles(col_template=\"{col_name}\", size=16)\ng.set_axis_labels(\"CHILDREN\", \"\")\nfor ax in g.axes.flatten():\n    ax.tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:40:18.987117Z","iopub.execute_input":"2025-07-05T18:40:18.987425Z","iopub.status.idle":"2025-07-05T18:40:20.708224Z","shell.execute_reply.started":"2025-07-05T18:40:18.987402Z","shell.execute_reply":"2025-07-05T18:40:20.707168Z"},"papermill":{"duration":50.16676,"end_time":"2025-04-13T21:18:34.25625","exception":false,"start_time":"2025-04-13T21:17:44.08949","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ff = ['bmi', 'age', 'children', 'premium']\n\ndf_melted = data.melt(id_vars=['expenses', 'gender'], value_vars=ff, var_name='variable', value_name='value')\n\ng = sns.relplot(\n    data=df_melted,\n    x='expenses',\n    y='value',\n    hue='gender',\n    col='variable',\n    kind='scatter',\n    palette=custom_palette2,\n    facet_kws={'sharey': False, 'sharex': True},\n    height=5,\n    aspect=1)\n\ng.set_titles(col_template=\"{col_name}\", size=16)\ng.set_axis_labels(\"EXPENSES\", \"\")\nfor ax in g.axes.flatten():\n    ax.tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2025-07-05T18:40:20.709612Z","iopub.execute_input":"2025-07-05T18:40:20.709903Z","iopub.status.idle":"2025-07-05T18:40:22.474Z","shell.execute_reply.started":"2025-07-05T18:40:20.709881Z","shell.execute_reply":"2025-07-05T18:40:22.473084Z"},"papermill":{"duration":46.455705,"end_time":"2025-04-13T21:19:21.394899","exception":false,"start_time":"2025-04-13T21:18:34.939194","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ff = ['bmi', 'age', 'children', 'expenses']\n\ndf_melted = data.melt(id_vars=['premium', 'gender'], value_vars=ff, var_name='variable', value_name='value')\n\ng = sns.relplot(\n    data=df_melted,\n    x='premium',\n    y='value',\n    hue='gender',\n    col='variable',\n    kind='scatter',\n    palette=custom_palette2,\n    facet_kws={'sharey': False, 'sharex': True},\n    height=5,\n    aspect=1)\n\ng.set_titles(col_template=\"{col_name}\", size=16)\ng.set_axis_labels(\"PREMIUM\", \"\")\nfor ax in g.axes.flatten():\n    ax.tick_params(axis='x', rotation=45)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:22.475216Z","iopub.execute_input":"2025-07-05T18:40:22.475462Z","iopub.status.idle":"2025-07-05T18:40:24.266457Z","shell.execute_reply.started":"2025-07-05T18:40:22.475442Z","shell.execute_reply":"2025-07-05T18:40:24.265412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sns.set(style=\"whitegrid\")\n\ncolumns = [\n    'age', 'gender', 'bmi', 'children',\n    'discount_eligibility', 'region', 'expenses', 'premium']\n\ncustom_palette = [\"#B7D5D4\", \"#77878B\", \"#488286\", \"#305252\", \"#92B8A7\", \"#3B5A54\"]\n\nn = len(columns)\nrows = (n + 1) // 2\nfig, axes = plt.subplots(rows, 2, figsize=(18, rows * 6))\n\nfor i, col in enumerate(columns):\n    ax = axes[i // 2, i % 2]\n    if data[col].dtype in ['int64', 'float64']: \n        sns.kdeplot(\n            data[col],\n            fill=True,\n            color=custom_palette[i % len(custom_palette)],\n            alpha=0.6,\n            linewidth=2,\n            ax=ax\n        )\n        ax.set_xlabel(col, fontsize=14)\n        ax.set_ylabel(\"Density\", fontsize=14)\n        ax.set_title(f'Distribution of {col}', fontsize=16)\n    else: \n        sns.countplot(\n            x=col,\n            data=data,\n            palette=custom_palette,\n            ax=ax\n        )\n        ax.set_xlabel(col, fontsize=14)\n        ax.set_ylabel(\"Count\", fontsize=14)\n        ax.set_title(f'Count Plot of {col}', fontsize=16)\n        for p in ax.patches:\n            height = p.get_height()\n            ax.annotate(f'{height}', (p.get_x() + p.get_width() / 2, height),\n                        ha='center', va='bottom', fontsize=11)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:24.26811Z","iopub.execute_input":"2025-07-05T18:40:24.26839Z","iopub.status.idle":"2025-07-05T18:40:26.816997Z","shell.execute_reply.started":"2025-07-05T18:40:24.268368Z","shell.execute_reply":"2025-07-05T18:40:26.815955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_cols = data.select_dtypes(include=['number']).columns\n\nskew_values = data[numeric_cols].skew(axis=0, skipna=True)\n\nprint(\"Skewness of Numeric Columns:\\n\")\nprint(skew_values.sort_values(ascending=False).round(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:26.818106Z","iopub.execute_input":"2025-07-05T18:40:26.818919Z","iopub.status.idle":"2025-07-05T18:40:26.828759Z","shell.execute_reply.started":"2025-07-05T18:40:26.81889Z","shell.execute_reply":"2025-07-05T18:40:26.827955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_mean = data[numeric_cols].mean()\n\ncustom_palette = [\"#B7D5D4\", \"#77878B\", \"#488286\", \"#305252\", \"#92B8A7\", \"#3B5A54\"]\n\ncolors_to_use = (custom_palette * ((len(data_mean) // len(custom_palette)) + 1))[:len(data_mean)]\n\ndata_mean.plot(\n    kind='barh',\n    figsize=(15, 5),\n    color=colors_to_use\n)\n\nplt.xlabel('Average')\nplt.title(\"Average of Numerical Columns\")\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:26.829899Z","iopub.execute_input":"2025-07-05T18:40:26.830607Z","iopub.status.idle":"2025-07-05T18:40:27.106807Z","shell.execute_reply.started":"2025-07-05T18:40:26.830582Z","shell.execute_reply":"2025-07-05T18:40:27.105885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data[numeric_cols].boxplot(figsize=(35,10),vert=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:27.108098Z","iopub.execute_input":"2025-07-05T18:40:27.10839Z","iopub.status.idle":"2025-07-05T18:40:27.572872Z","shell.execute_reply.started":"2025-07-05T18:40:27.108367Z","shell.execute_reply":"2025-07-05T18:40:27.570977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_cols = data.select_dtypes(include=['number']).columns\n\nfor i, column in enumerate(numeric_cols):\n    plt.figure(figsize=(15, 3))\n    sns.violinplot(\n        x=data[column],\n        palette=[custom_palette[i % len(custom_palette)]], \n        inner=\"quartile\"\n    )\n    plt.title(f'Distribution of {column}', fontsize=18)\n    plt.xlabel(column, fontsize=14)\n    plt.ylabel('Density', fontsize=14)\n    plt.grid(True)\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:27.574139Z","iopub.execute_input":"2025-07-05T18:40:27.574542Z","iopub.status.idle":"2025-07-05T18:40:28.6588Z","shell.execute_reply.started":"2025-07-05T18:40:27.574515Z","shell.execute_reply":"2025-07-05T18:40:28.657885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, column in enumerate(ff):\n    plt.figure(figsize=(15, 3))\n    sns.boxplot(\n        x=data[column],\n        color=custom_palette[i % len(custom_palette)])\n    \n    plt.title(f'Distribution of {column}', fontsize=18)\n    plt.xlabel(column, fontsize=14)\n\n    stats = data[column].describe()\n    stats_text = \"\\n\".join([f\"{key}: {value:.2f}\" for key, value in stats.items()])\n    plt.text(\n        0.95, 0.95, stats_text,\n        transform=plt.gca().transAxes,\n        fontsize=12,\n        verticalalignment='top',\n        horizontalalignment='center',\n        bbox=dict(boxstyle='round,pad=0.2', edgecolor='black', facecolor='lightgrey'))\n\n    plt.grid(True)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:28.659802Z","iopub.execute_input":"2025-07-05T18:40:28.660204Z","iopub.status.idle":"2025-07-05T18:40:29.565678Z","shell.execute_reply.started":"2025-07-05T18:40:28.660182Z","shell.execute_reply":"2025-07-05T18:40:29.564736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"count_vars = [\n    'gender',\n    'children',\n    'discount_eligibility',\n    'region']\nn_cols = 2\nn_rows = (len(count_vars) + n_cols - 1) // n_cols\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 10), dpi=90)\naxes = axes.flatten()\nfor i, column in enumerate(count_vars):\n    sns.countplot(\n        x=column,\n        data=data,\n        palette=custom_palette,\n        hue='gender',\n        ax=axes[i]\n    )\n    axes[i].set_title(f'Count of {column}', fontsize=18)\n    axes[i].set_xlabel(column, fontsize=14)\n    axes[i].set_ylabel('Count', fontsize=14)\n    axes[i].tick_params(axis='x', rotation=20, labelsize=12)\n    axes[i].tick_params(axis='y', labelsize=12)\n    axes[i].grid(True, linestyle='--', alpha=0.6)\n    axes[i].legend(title='Gender', fontsize=11, title_fontsize=12)\nfor j in range(i + 1, len(axes)):\n    fig.delaxes(axes[j])\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:29.568187Z","iopub.execute_input":"2025-07-05T18:40:29.568468Z","iopub.status.idle":"2025-07-05T18:40:30.533569Z","shell.execute_reply.started":"2025-07-05T18:40:29.568446Z","shell.execute_reply":"2025-07-05T18:40:30.532427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"metrics = [\n    ('age', 'Age'),\n    ('bmi', 'BMI'),\n    ('children', 'Children'),\n    ('expenses', 'Expenses'),\n    ('premium', 'Premium')]\n\ngroup_col = 'region' \n\nfig, axes = plt.subplots(3, 2, figsize=(25, 15))\nfig.suptitle(f'Trends by {group_col.title()}', fontsize=30)\n\nfor i, (col, label) in enumerate(metrics):\n    row, col_pos = divmod(i, 2)\n    sns.lineplot(\n        ax=axes[row, col_pos],\n        x=group_col,\n        y=col,\n        data=data,\n        ci=None,\n        color=custom_palette[i % len(custom_palette)],\n        linewidth=2\n    )\n    axes[row, col_pos].set_title(f'{label} by {group_col.title()}', fontsize=20)\n    axes[row, col_pos].set_xlabel(group_col.title(), fontsize=16)\n    axes[row, col_pos].set_ylabel(label, fontsize=16)\n    axes[row, col_pos].tick_params(axis='x', rotation=45, labelsize=14)\n    axes[row, col_pos].tick_params(axis='y', labelsize=14)\n    axes[row, col_pos].grid(True, linestyle='--', alpha=0.7)\nif len(metrics) < 6:\n    fig.delaxes(axes[2, 1])  \n\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:30.534884Z","iopub.execute_input":"2025-07-05T18:40:30.53525Z","iopub.status.idle":"2025-07-05T18:40:31.977523Z","shell.execute_reply.started":"2025-07-05T18:40:30.535219Z","shell.execute_reply":"2025-07-05T18:40:31.976553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"custom_palette = [\"#B7D5D4\", \"#77878B\", \"#488286\", \"#305252\", \"#92B8A7\", \"#3B5A54\"]\n\nfig, axes = plt.subplots(2, 1, figsize=(25, 15))\n\nsns.stripplot(\n    data=data,\n    x='expenses',\n    y='region',\n    hue='children',\n    palette=custom_palette,\n    orient='h',\n    ax=axes[0],\n    dodge=True\n)\naxes[0].set_title('Expenses by Region and Children Count', fontsize=16)\naxes[0].legend(loc='lower right', title='Children')\n\n# Boxplot\nsns.boxplot(\n    data=data,\n    x='expenses',\n    y='region',\n    palette=custom_palette,\n    orient='h',\n    ax=axes[1]\n)\naxes[1].set_title('Distribution of Expenses by Region', fontsize=16)\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:31.978725Z","iopub.execute_input":"2025-07-05T18:40:31.979148Z","iopub.status.idle":"2025-07-05T18:40:32.917736Z","shell.execute_reply.started":"2025-07-05T18:40:31.979124Z","shell.execute_reply":"2025-07-05T18:40:32.916693Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def StDev_method(data, n, features):\n    outliers = []\n    total_outliers = 0\n    \n    for column in features:\n        data_mean = data[column].mean()\n        data_std = data[column].std()\n        cut_off = data_std * 3\n\n        high_outliers = data[data[column] > data_mean + cut_off].index\n        low_outliers = data[data[column] < data_mean - cut_off].index\n\n        outliers.extend(high_outliers)\n        outliers.extend(low_outliers)\n\n        print(f\"{column}: {len(high_outliers) + len(low_outliers)} outliers\")\n\n        total_outliers += len(high_outliers) + len(low_outliers)\n\n    outliers_counter = Counter(outliers)\n    OUT = [idx for idx, count in outliers_counter.items() if count > n]\n\n    print(f'\\nüîé Total unique outlier rows (appearing in > {n} features): {len(OUT)}')\n    print(f'üìå Total individual outlier points detected: {total_outliers}')\n    \n    return OUT\n\nOutliers_StDev = StDev_method(data, 1, ff)\n\ndata_out = data.drop(Outliers_StDev, axis=0).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:32.918792Z","iopub.execute_input":"2025-07-05T18:40:32.919086Z","iopub.status.idle":"2025-07-05T18:40:32.934424Z","shell.execute_reply.started":"2025-07-05T18:40:32.919065Z","shell.execute_reply":"2025-07-05T18:40:32.933445Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gender_dummies = pd.get_dummies(data_out['gender'], dtype=int, drop_first=True)\ndiscount_eligibility_dummies = pd.get_dummies(data_out['discount_eligibility'], dtype=int, drop_first=True)\nregion_dummies = pd.get_dummies(data_out['region'], dtype=int, drop_first=False)\n\ndata_out = pd.concat([data_out.iloc[:, :-1], region_dummies, data_out.iloc[:, -1]], axis=1)\ndata_out.drop(columns=['region'], inplace=True)\ndata_out['gender'] = gender_dummies\ndata_out['discount_eligibility'] = discount_eligibility_dummies\n\ndata_out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:32.935396Z","iopub.execute_input":"2025-07-05T18:40:32.935714Z","iopub.status.idle":"2025-07-05T18:40:32.976056Z","shell.execute_reply.started":"2025-07-05T18:40:32.935691Z","shell.execute_reply":"2025-07-05T18:40:32.975182Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(20, 12))\nsns.heatmap(\n    data_out.corr(numeric_only=True),\n    cmap=custom_cmap,\n    annot=True,\n    linewidths=0.6,\n    cbar=False)\n\nplt.xticks(rotation=60, size=10)\nplt.yticks(size=10)\nplt.title('Analysis of Correlations', size=20)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T18:40:32.976889Z","iopub.execute_input":"2025-07-05T18:40:32.977141Z","iopub.status.idle":"2025-07-05T18:40:33.654846Z","shell.execute_reply.started":"2025-07-05T18:40:32.977122Z","shell.execute_reply":"2025-07-05T18:40:33.65399Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<a id=\"2\"></a>\n\n<h1 style=\"\n    background-color:#488286; \n    background-size: cover;\n    background-position: center;\n    font-family: 'Times New Roman', serif;\n    font-size: 1.5em;\n    color: white;\n    text-align: center;\n    padding: 15px;\n    border-radius: 15px 50px;\n    border: 1px solid black;\n\">\n    Modeling\n</h1>","metadata":{}},{"cell_type":"markdown","source":"### <span style=\"color: #305252;\">üîç Regression Modeling Pipeline Overview</span>\n\nüìä **1. Data Preprocessing**\n\n* Applied `pd.get_dummies()` to convert categorical variables into numerical format.\n* Used `StandardScaler` to normalize numerical features for consistent scaling.\n\nüß† **2. Regression Models (14 total), grouped as:**\n\n* **Linear Models:** Ridge, Lasso, ElasticNet, BayesianRidge, HuberRegressor\n* **Tree-Based Model:** DecisionTreeRegressor\n* **Ensemble Trees:** RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n* **Nearest Neighbors:** KNeighborsRegressor\n* **Kernel-Based:** Support Vector Regressor (SVR)\n* **Advanced Gradient Boosting:** XGBoost, LightGBM, CatBoost\n\nüîß **3. Hyperparameter Tuning**\n\n* Employed `GridSearchCV` with specified parameter grids for all models.\n* Used 5-fold cross-validation (`cv=5`).\n* Optimized based on negative mean squared error (`neg_mean_squared_error`).\n* Enabled parallel processing with `n_jobs=-1` for efficiency.\n\nüìà **4. Model Evaluation Metrics**\n\n* Evaluated models on training and testing data with:\n\n  * Mean Absolute Error (MAE)\n  * Mean Squared Error (MSE)\n  * Coefficient of Determination (R¬≤)\n  * Adjusted R¬≤\n\nüìä **5. Final Visualizations**\n\n* MAE comparison: Train vs Test\n* R¬≤ comparison: Train vs Test\n* Histogram of Actual-to-Predicted value ratios\n* Residual distribution across models\n* Actual vs Predicted scatter plots for each model","metadata":{}},{"cell_type":"markdown","source":"## - Forecasting Expenses","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\n# Custom color palette for visualization\ncustom_palette = ['#B7D5D4', '#77878B', '#488286', '#305252']\n\n# Select feature columns and target variable\nfeatures = ['age', 'gender', 'bmi', 'children', 'discount_eligibility',\n            'northeast', 'northwest', 'southeast', 'southwest']\ntarget = 'expenses' \n\nX = data_out[features]\ny = data_out[target]\n\nX = pd.get_dummies(X, columns=['gender', 'discount_eligibility'], drop_first=True)\n\n# Split the dataset into training and testing subsets with random shuffling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and apply standard scaler to numerical features for normalization\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Dictionary of candidate regression models with default hyperparameters\nmodel_candidates = {\n    'Ridge Regression': Ridge(),\n    'Lasso Regression': Lasso(),\n    'ElasticNet': ElasticNet(),\n    'Bayesian Ridge': BayesianRidge(),\n    'Huber Regressor': HuberRegressor(),\n    'Decision Tree': DecisionTreeRegressor(),\n    'Random Forest': RandomForestRegressor(),\n    'Gradient Boosting': GradientBoostingRegressor(),\n    'AdaBoost': AdaBoostRegressor(),\n    'K-Nearest Neighbors': KNeighborsRegressor(),\n    'Support Vector Regressor': SVR(),\n    'XGBoost': XGBRegressor(verbosity=0), \n    'LightGBM': LGBMRegressor(verbose=-1),  \n    'CatBoost': CatBoostRegressor(verbose=0) \n}\n\n# Hyperparameter grids for GridSearch cross-validation\nparam_grids = {\n    'Ridge Regression': {'alpha': [0.1, 1.0, 10.0]},\n    'Lasso Regression': {'alpha': [0.1, 1.0, 10.0]},\n    'ElasticNet': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n    'Bayesian Ridge': {\n        'alpha_1': [1e-6, 1e-5, 1e-4], 'alpha_2': [1e-6, 1e-5, 1e-4],\n        'lambda_1': [1e-6, 1e-5, 1e-4], 'lambda_2': [1e-6, 1e-5, 1e-4]\n    },\n    'Huber Regressor': {'alpha': [0.0001, 0.001, 0.01]},\n    'Decision Tree': {'max_depth': [None, 10, 20, 30]},\n    'Random Forest': {'n_estimators': [100, 200, 500], 'max_depth': [None, 10, 20, 30]},\n    'Gradient Boosting': {'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]},\n    'AdaBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n    'K-Nearest Neighbors': {'n_neighbors': [3, 5, 10], 'weights': ['uniform', 'distance']},\n    'Support Vector Regressor': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1.0, 10.0], 'epsilon': [0.01, 0.1, 0.2]},\n    'XGBoost': {'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]},\n    'LightGBM': {'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]},\n    'CatBoost': {'iterations': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]}\n}\n\n# Helper function to compute adjusted R¬≤\ndef adjusted_r2(r2, n_samples, n_features):\n    return 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\n\n# Initialize dictionary to store results and predictions\nresults = {\n    'Model': [],\n    'MSE_Train': [],\n    'R2_Train': [],\n    'Adj_R2_Train': [],\n    'MAE_Train': [],\n    'MSE_Test': [],\n    'R2_Test': [],\n    'Adj_R2_Test': [],\n    'MAE_Test': [],\n    'y_test_true': [],\n    'y_test_pred': [],\n    'y_train_true': [],\n    'y_train_pred': []\n}\n\n# Train, tune, predict, and evaluate models\nfor name, model in model_candidates.items():\n    print(f\"Training and tuning {name}...\")\n\n    if name in param_grids:\n        grid_search = GridSearchCV(model, param_grids[name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=0)\n        grid_search.fit(X_train_scaled, y_train) \n        best_model = grid_search.best_estimator_\n    else:\n        best_model = model\n        best_model.fit(X_train_scaled, y_train) \n\n    y_train_pred = best_model.predict(X_train_scaled)\n    y_test_pred = best_model.predict(X_test_scaled)\n\n    mse_train = mean_squared_error(y_train, y_train_pred)\n    r2_train = r2_score(y_train, y_train_pred)\n    adj_r2_train = adjusted_r2(r2_train, len(y_train), X_train_scaled.shape[1])\n    mae_train = mean_absolute_error(y_train, y_train_pred)\n\n    mse_test = mean_squared_error(y_test, y_test_pred)\n    r2_test = r2_score(y_test, y_test_pred)\n    adj_r2_test = adjusted_r2(r2_test, len(y_test), X_test_scaled.shape[1])\n    mae_test = mean_absolute_error(y_test, y_test_pred)\n\n    results['Model'].append(name)\n    results['MSE_Train'].append(mse_train)\n    results['R2_Train'].append(r2_train)\n    results['Adj_R2_Train'].append(adj_r2_train)\n    results['MAE_Train'].append(mae_train)\n    results['MSE_Test'].append(mse_test)\n    results['R2_Test'].append(r2_test)\n    results['Adj_R2_Test'].append(adj_r2_test)\n    results['MAE_Test'].append(mae_test)\n    results['y_test_true'].append(y_test)\n    results['y_test_pred'].append(y_test_pred)\n    results['y_train_true'].append(y_train)\n    results['y_train_pred'].append(y_train_pred)\n\n# Convert to DataFrame and sort by test R2\nresults_df = pd.DataFrame(results).sort_values(by='R2_Test', ascending=False).reset_index(drop=True)\n\n# --------- Plot MAE Train vs Test --------------\nplt.figure(figsize=(12,6))\nsns.scatterplot(x='MAE_Train', y='MAE_Test', data=results_df, hue='Model', palette=custom_palette, s=100)\nplt.plot([results_df['MAE_Train'].min(), results_df['MAE_Train'].max()],\n         [results_df['MAE_Train'].min(), results_df['MAE_Train'].max()],\n         'r--', lw=2)\nplt.title('Mean Absolute Error (Train vs Test)', fontsize=16)\nplt.xlabel('MAE Train')\nplt.ylabel('MAE Test')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()\n\n# --------- Plot R2 Train vs Test --------------\nplt.figure(figsize=(12,6))\nsns.scatterplot(x='R2_Train', y='R2_Test', data=results_df, hue='Model', palette=custom_palette, s=100)\nplt.plot([results_df['R2_Train'].min(), results_df['R2_Train'].max()],\n         [results_df['R2_Train'].min(), results_df['R2_Train'].max()],\n         'r--', lw=2)\nplt.title('R¬≤ Score (Train vs Test)', fontsize=16)\nplt.xlabel('R¬≤ Train')\nplt.ylabel('R¬≤ Test')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()\n\n# --------- Scatter plot of Actual/Predicted Ratio (Test data) --------------\nplt.figure(figsize=(14,8))\nratios = []\nmodels_expanded = []\nfor i, row in results_df.iterrows():\n    ratio = row['y_test_true'] / (row['y_test_pred'] + 1e-8)  # Avoid div by zero\n    ratios.extend(ratio)\n    models_expanded.extend([row['Model']]*len(ratio))\n\ndf_ratio = pd.DataFrame({'Ratio': ratios, 'Model': models_expanded})\nsns.histplot(data=df_ratio, x='Ratio', hue='Model', element='step', stat='count', palette=custom_palette, bins=30)\nplt.title('Distribution of Actual/Predicted Ratio by Model (Test Data)', fontsize=16)\nplt.xlabel('Actual / Predicted')\nplt.ylabel('Count')\nplt.grid(True)\nplt.show()\n\n# --------- Residuals distribution plot (Test data) --------------\nplt.figure(figsize=(14,8))\nfor i, row in results_df.iterrows():\n    residuals = row['y_test_true'] - row['y_test_pred']\n    sns.kdeplot(residuals, label=row['Model'], fill=True, alpha=0.3)\nplt.title('Residuals Distribution (Test Data)', fontsize=16)\nplt.xlabel('Residuals (Actual - Predicted)')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()\n\n# --------- Actual vs Predicted plot for all models --------------\nfig, axs = plt.subplots(len(results_df), 1, figsize=(10, len(results_df)*4), sharex=True)\nif len(results_df) == 1:\n    axs = [axs]\n\nfor i, row in results_df.iterrows():\n    sns.scatterplot(x=row['y_test_true'], y=row['y_test_pred'], ax=axs[i], color=custom_palette[i % len(custom_palette)], alpha=0.6)\n    axs[i].plot([row['y_test_true'].min(), row['y_test_true'].max()], [row['y_test_true'].min(), row['y_test_true'].max()], 'r--')\n    axs[i].set_title(f'Actual vs Predicted Premium - {row[\"Model\"]}')\n    axs[i].set_xlabel('Actual Premium')\n    axs[i].set_ylabel('Predicted Premium')\n    axs[i].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Custom palette\ncustom_palette = ['#B7D5D4', '#77878B', '#488286', '#305252']\ncustom_cmap = LinearSegmentedColormap.from_list(\"custom\", custom_palette)\n\n# --------- Styled Performance Summary Table ---------\nstyled_results = results_df[['Model', 'MSE_Train', 'MAE_Train', 'R2_Train', 'Adj_R2_Train',\n                             'MSE_Test', 'MAE_Test', 'R2_Test', 'Adj_R2_Test']].style\\\n    .background_gradient(cmap=custom_cmap, axis=1)\\\n    .format({\n        'MSE_Train': \"{:.2f}\",\n        'MAE_Train': \"{:.2f}\",\n        'R2_Train': \"{:.3f}\",\n        'Adj_R2_Train': \"{:.3f}\",\n        'MSE_Test': \"{:.2f}\",\n        'MAE_Test': \"{:.2f}\",\n        'R2_Test': \"{:.3f}\",\n        'Adj_R2_Test': \"{:.3f}\",\n    })\\\n    .set_caption(\"Comprehensive Regression Models Performance Summary - Target: Expenses\")\n\ndisplay(styled_results)\n\n\n# Additional line plot for train vs test R2 scores\nplt.figure(figsize=(14, 7))\ndf_long = pd.melt(results_df[['Model', 'R2_Train', 'R2_Test']], id_vars='Model', \n                  value_vars=['R2_Train', 'R2_Test'],\n                  var_name='Dataset', value_name='R2_Score')\n\npalette = ['#488286', '#B7D5D4'] \n\nsns.lineplot(data=df_long, x='Model', y='R2_Score', hue='Dataset', marker='o', palette=palette)\nplt.xticks(rotation=45, fontsize=12)\nplt.title('R¬≤ Score Comparison: Train vs Test for All Models')\nplt.xlabel('Model')\nplt.ylabel('R¬≤ Score')\nplt.legend(title='Dataset')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Additional line plot for MAE comparison\nplt.figure(figsize=(14, 6))\nplt.plot(results_df['Model'], results_df['MAE_Train'], marker='o', label='Train MAE', color='#B7D5D4')\nplt.plot(results_df['Model'], results_df['MAE_Test'], marker='o', label='Test MAE', color='#305252')\nplt.xticks(rotation=45, ha='right')\nplt.title('MAE Comparison Across Models')\nplt.xlabel('Models')\nplt.ylabel('Mean Absolute Error')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T19:01:57.542414Z","iopub.execute_input":"2025-07-05T19:01:57.542757Z","iopub.status.idle":"2025-07-05T19:02:59.858216Z","shell.execute_reply.started":"2025-07-05T19:01:57.54273Z","shell.execute_reply":"2025-07-05T19:02:59.857184Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"font-weight:bold; font-size:1.3em;\">‚úÖ 1. Top Performing Models</span>\n\n| Model               | R¬≤ Test   | MAE Test | Overfitting Assessment                                                 |\n| ------------------- | --------- | -------- | ---------------------------------------------------------------------- |\n| ‚úÖ Gradient Boosting | **0.901** | 2494.13  | ‚ùå Minimal overfitting; train and test R¬≤ are closely aligned          |\n| ‚úÖ XGBoost           | **0.898** | 2393.28  | ‚ö†Ô∏è Slight overfitting; excellent train R¬≤ (0.932) with small test drop |\n| ‚úÖ LightGBM          | **0.897** | 2462.08  | ‚ö†Ô∏è Slight overfitting; solid train R¬≤ (0.910) and stable test accuracy  |\n| ‚úÖ CatBoost          | **0.896** | 2564.63  | ‚ö†Ô∏è Slight overfitting; comparable train/test R¬≤, slightly higher MAE   |\n| ‚úÖ AdaBoost          | **0.892** | 2936.20  | ‚ö†Ô∏è Mild overfitting; higher MAE than other boosting models             |\n| ‚úÖ Random Forest     | **0.883** | 2591.72  | ‚ö†Ô∏è Noticeable overfitting; very high train R¬≤ (0.966) vs test R¬≤        |\n\n**Key Insights:**\n\n* **Gradient Boosting** delivers best test R¬≤ with balanced low MAE and minimal overfitting.\n* **XGBoost** and **LightGBM** provide strong accuracy with slightly better train fits.\n* **CatBoost** remains competitive, especially in handling categorical data.\n* **AdaBoost** shows stable performance but with relatively higher errors.\n* **Random Forest** has excellent train performance but suffers from overfitting.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">‚ö†Ô∏è 2. Highly Overfitted Models</span>\n\n| Model                 | R¬≤ Train | R¬≤ Test | Overfitting Severity                                                   |\n| --------------------- | -------- | ------- | --------------------------------------------------------------------- |\n| ‚ùå K-Nearest Neighbors | 1.000    | 0.819   | ‚úÖ Severe overfitting; perfect train fit, large drop on test          |\n| ‚ùå Decision Tree       | 0.984    | 0.789   | ‚úÖ Significant overfitting; poor generalization despite low train error|\n\n**Summary:**\n\n* These models drastically overfit training data but fail to generalize well.\n* Use with pruning, regularization, or ensemble techniques to improve.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">‚õî 3. Underperforming Models</span>\n\n| Model                          | R¬≤ Test | MAE Test |\n| ------------------------------ | ------- | -------- |\n| Support Vector Regressor (SVR) | 0.467   | 4871.32  |\n| Huber Regressor                | 0.791   | 3355.37  |\n| Lasso, Ridge, ElasticNet       | ~0.805  | ~4200    |\n| Bayesian Ridge                 | 0.806   | 4184.80  |\n\n**Observations:**\n\n* These models struggle with the dataset‚Äôs nonlinear and complex patterns.\n* High error and low R¬≤ indicate limited suitability for premium prediction.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">‚ö° 4. Special Case: AdaBoost</span>\n\n| Model    | R¬≤ Train | R¬≤ Test | MAE Test |\n| -------- | -------- | ------- | -------- |\n| AdaBoost | 0.848    | 0.892   | 2936.20  |\n\n* AdaBoost shows consistent behavior with relatively small train-test R¬≤ gap.\n* Slightly weaker than other boosting methods in accuracy and MAE.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">üí° Final Recommendations</span>\n\n**Top recommended models:**\n\n* `Gradient Boosting`: Best trade-off between accuracy and generalization.\n* `XGBoost` and `LightGBM`: Highly effective and fast alternatives.\n* `CatBoost`: Strong option for categorical feature handling.\n\n**Models to avoid:**\n\n* `K-Nearest Neighbors` and `Decision Tree`: Overfitting issues, poor test performance.\n* `Support Vector Regressor`, `Huber`, and linear models: Low accuracy and high errors.\n\n","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\nbest_models = {} \n\nfor name, model in model_candidates.items():\n    print(f\"Training and tuning {name}...\")\n    if name in param_grids:\n        grid_search = GridSearchCV(model, param_grids[name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=0)\n        grid_search.fit(X_train_scaled, y_train)\n        best_model = grid_search.best_estimator_\n    else:\n        best_model = model\n        best_model.fit(X_train_scaled, y_train)\n    \n    best_models[name] = best_model\n    \n# Example input \ninput_data = {\n    'age': 28,\n    'gender': 1,  # 1 for male, 0 for female\n    'bmi': 33.0,\n    'children': 3,\n    'discount_eligibility': 0,\n    'northeast': 0,\n    'northwest': 0,\n    'southeast': 1,\n    'southwest': 0,\n}\n\n# Convert to DataFrame\ninput_df = pd.DataFrame([input_data])\n\n# One-hot encode categorical columns, as done in training\ninput_df = pd.get_dummies(input_df, columns=['gender', 'discount_eligibility'], drop_first=True)\n\n# Ensure all expected columns exist (add missing dummy columns if any)\nexpected_cols = X.columns.tolist()  # X is our original training features DataFrame\nfor col in expected_cols:\n    if col not in input_df.columns:\n        input_df[col] = 0\n\n# Reorder columns to match training data\ninput_df = input_df[expected_cols]\n\n# Scale input features using the same scaler as training\ninput_scaled = scaler.transform(input_df)\n\n# Predict with each best model (change this list as needed)\nfor model_name in ['Gradient Boosting', 'XGBoost', 'LightGBM', 'AdaBoost', 'CatBoost']:\n    model = best_models[model_name]  # Assuming we saved best models after grid search\n    pred = model.predict(input_scaled)\n    print(f'\\033[96m {model_name} prediction: {pred[0]:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T19:04:35.16017Z","iopub.execute_input":"2025-07-05T19:04:35.160491Z","iopub.status.idle":"2025-07-05T19:05:36.257962Z","shell.execute_reply.started":"2025-07-05T19:04:35.160467Z","shell.execute_reply":"2025-07-05T19:05:36.257077Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## - Forecasting Premiums","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\n# Custom color palette for visualization\ncustom_palette = ['#B7D5D4', '#77878B', '#488286', '#305252']\n\n# Select feature columns and target variable (premium as target)\nfeatures = ['age', 'gender', 'bmi', 'children', 'discount_eligibility',\n            'northeast', 'northwest', 'southeast', 'southwest']\ntarget = 'premium' \n\nX = data_out[features]\ny = data_out[target]\n\n# One-hot encoding categorical columns\nX = pd.get_dummies(X, columns=['gender', 'discount_eligibility'], drop_first=True)\n\n# Split dataset into training and testing subsets with random shuffling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Standardize numerical features\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Candidate regression models with default hyperparameters\nmodel_candidates = {\n    'Ridge Regression': Ridge(),\n    'Lasso Regression': Lasso(),\n    'ElasticNet': ElasticNet(),\n    'Bayesian Ridge': BayesianRidge(),\n    'Huber Regressor': HuberRegressor(),\n    'Decision Tree': DecisionTreeRegressor(),\n    'Random Forest': RandomForestRegressor(),\n    'Gradient Boosting': GradientBoostingRegressor(),\n    'AdaBoost': AdaBoostRegressor(),\n    'K-Nearest Neighbors': KNeighborsRegressor(),\n    'Support Vector Regressor': SVR(),\n    'XGBoost': XGBRegressor(verbosity=0), \n    'LightGBM': LGBMRegressor(verbose=-1),  \n    'CatBoost': CatBoostRegressor(verbose=0)\n}\n\n# Hyperparameter grids for GridSearchCV\nparam_grids = {\n    'Ridge Regression': {'alpha': [0.1, 1.0, 10.0]},\n    'Lasso Regression': {'alpha': [0.1, 1.0, 10.0]},\n    'ElasticNet': {'alpha': [0.1, 1.0, 10.0], 'l1_ratio': [0.1, 0.5, 0.9]},\n    'Bayesian Ridge': {\n        'alpha_1': [1e-6, 1e-5, 1e-4], 'alpha_2': [1e-6, 1e-5, 1e-4],\n        'lambda_1': [1e-6, 1e-5, 1e-4], 'lambda_2': [1e-6, 1e-5, 1e-4]\n    },\n    'Huber Regressor': {'alpha': [0.0001, 0.001, 0.01]},\n    'Decision Tree': {'max_depth': [None, 10, 20, 30]},\n    'Random Forest': {'n_estimators': [100, 200, 500], 'max_depth': [None, 10, 20, 30]},\n    'Gradient Boosting': {'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]},\n    'AdaBoost': {'n_estimators': [50, 100, 200], 'learning_rate': [0.01, 0.1, 0.2]},\n    'K-Nearest Neighbors': {'n_neighbors': [3, 5, 10], 'weights': ['uniform', 'distance']},\n    'Support Vector Regressor': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1.0, 10.0], 'epsilon': [0.01, 0.1, 0.2]},\n    'XGBoost': {'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]},\n    'LightGBM': {'n_estimators': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]},\n    'CatBoost': {'iterations': [100, 200, 500], 'learning_rate': [0.01, 0.1, 0.2]}\n}\n\n# Adjusted R¬≤ calculation helper\ndef adjusted_r2(r2, n_samples, n_features):\n    return 1 - (1 - r2) * (n_samples - 1) / (n_samples - n_features - 1)\n\n# Initialize storage for results\nresults = {\n    'Model': [],\n    'MSE_Train': [],\n    'R2_Train': [],\n    'Adj_R2_Train': [],\n    'MAE_Train': [],\n    'MSE_Test': [],\n    'R2_Test': [],\n    'Adj_R2_Test': [],\n    'MAE_Test': [],\n    'y_test_true': [],\n    'y_test_pred': [],\n    'y_train_true': [],\n    'y_train_pred': []\n}\n\n# Train, tune, predict, and evaluate each model\nfor name, model in model_candidates.items():\n    print(f\"Training and tuning {name}...\")\n    if name in param_grids:\n        grid_search = GridSearchCV(model, param_grids[name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=0)\n        grid_search.fit(X_train_scaled, y_train)\n        best_model = grid_search.best_estimator_\n    else:\n        best_model = model\n        best_model.fit(X_train_scaled, y_train)\n\n    y_train_pred = best_model.predict(X_train_scaled)\n    y_test_pred = best_model.predict(X_test_scaled)\n\n    mse_train = mean_squared_error(y_train, y_train_pred)\n    r2_train = r2_score(y_train, y_train_pred)\n    adj_r2_train = adjusted_r2(r2_train, len(y_train), X_train_scaled.shape[1])\n    mae_train = mean_absolute_error(y_train, y_train_pred)\n\n    mse_test = mean_squared_error(y_test, y_test_pred)\n    r2_test = r2_score(y_test, y_test_pred)\n    adj_r2_test = adjusted_r2(r2_test, len(y_test), X_test_scaled.shape[1])\n    mae_test = mean_absolute_error(y_test, y_test_pred)\n\n    results['Model'].append(name)\n    results['MSE_Train'].append(mse_train)\n    results['R2_Train'].append(r2_train)\n    results['Adj_R2_Train'].append(adj_r2_train)\n    results['MAE_Train'].append(mae_train)\n    results['MSE_Test'].append(mse_test)\n    results['R2_Test'].append(r2_test)\n    results['Adj_R2_Test'].append(adj_r2_test)\n    results['MAE_Test'].append(mae_test)\n    results['y_test_true'].append(y_test)\n    results['y_test_pred'].append(y_test_pred)\n    results['y_train_true'].append(y_train)\n    results['y_train_pred'].append(y_train_pred)\n\n# Convert results to DataFrame and sort by test R¬≤\nresults_df = pd.DataFrame(results).sort_values(by='R2_Test', ascending=False).reset_index(drop=True)\n\n# Visualizations\nplt.figure(figsize=(12,6))\nsns.scatterplot(x='MAE_Train', y='MAE_Test', data=results_df, hue='Model', palette=custom_palette, s=100)\nplt.plot([results_df['MAE_Train'].min(), results_df['MAE_Train'].max()],\n         [results_df['MAE_Train'].min(), results_df['MAE_Train'].max()],\n         'r--', lw=2)\nplt.title('Mean Absolute Error (Train vs Test)')\nplt.xlabel('MAE Train')\nplt.ylabel('MAE Test')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(12,6))\nsns.scatterplot(x='R2_Train', y='R2_Test', data=results_df, hue='Model', palette=custom_palette, s=100)\nplt.plot([results_df['R2_Train'].min(), results_df['R2_Train'].max()],\n         [results_df['R2_Train'].min(), results_df['R2_Train'].max()],\n         'r--', lw=2)\nplt.title('R¬≤ Score (Train vs Test)')\nplt.xlabel('R¬≤ Train')\nplt.ylabel('R¬≤ Test')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(14,8))\nratios = []\nmodels_expanded = []\nfor i, row in results_df.iterrows():\n    ratio = row['y_test_true'] / (row['y_test_pred'] + 1e-8)  # prevent division by zero\n    ratios.extend(ratio)\n    models_expanded.extend([row['Model']]*len(ratio))\n\ndf_ratio = pd.DataFrame({'Ratio': ratios, 'Model': models_expanded})\nsns.histplot(data=df_ratio, x='Ratio', hue='Model', element='step', stat='count', palette=custom_palette, bins=30)\nplt.title('Distribution of Actual/Predicted Ratio by Model (Test Data)')\nplt.xlabel('Actual / Predicted')\nplt.ylabel('Count')\nplt.grid(True)\nplt.show()\n\nplt.figure(figsize=(14,8))\nfor i, row in results_df.iterrows():\n    residuals = row['y_test_true'] - row['y_test_pred']\n    sns.kdeplot(residuals, label=row['Model'], fill=True, alpha=0.3)\nplt.title('Residuals Distribution (Test Data)')\nplt.xlabel('Residuals (Actual - Predicted)')\nplt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\nplt.grid(True)\nplt.show()\n\nfig, axs = plt.subplots(len(results_df), 1, figsize=(10, len(results_df)*4), sharex=True)\nif len(results_df) == 1:\n    axs = [axs]\n\nfor i, row in results_df.iterrows():\n    sns.scatterplot(x=row['y_test_true'], y=row['y_test_pred'], ax=axs[i], color=custom_palette[i % len(custom_palette)], alpha=0.6)\n    axs[i].plot([row['y_test_true'].min(), row['y_test_true'].max()], [row['y_test_true'].min(), row['y_test_true'].max()], 'r--')\n    axs[i].set_title(f'Actual vs Predicted Premium - {row[\"Model\"]}')\n    axs[i].set_xlabel('Actual Premium')\n    axs[i].set_ylabel('Predicted Premium')\n    axs[i].grid(True)\n\nplt.tight_layout()\nplt.show()\n\n# Color map for table styling\ncustom_cmap = LinearSegmentedColormap.from_list(\"custom\", custom_palette)\n\n# Styled summary table\nstyled_results = results_df[['Model', 'MSE_Train', 'MAE_Train', 'R2_Train', 'Adj_R2_Train',\n                             'MSE_Test', 'MAE_Test', 'R2_Test', 'Adj_R2_Test']].style\\\n    .background_gradient(cmap=custom_cmap, axis=1)\\\n    .format({\n        'MSE_Train': \"{:.2f}\",\n        'MAE_Train': \"{:.2f}\",\n        'R2_Train': \"{:.3f}\",\n        'Adj_R2_Train': \"{:.3f}\",\n        'MSE_Test': \"{:.2f}\",\n        'MAE_Test': \"{:.2f}\",\n        'R2_Test': \"{:.3f}\",\n        'Adj_R2_Test': \"{:.3f}\",\n    })\\\n    .set_caption(\"Comprehensive Regression Models Performance Summary - Target: Premium\")\n\ndisplay(styled_results)\n\n# Additional line plot for train vs test R2 scores\nplt.figure(figsize=(14, 7))\ndf_long = pd.melt(results_df[['Model', 'R2_Train', 'R2_Test']], id_vars='Model', \n                  value_vars=['R2_Train', 'R2_Test'],\n                  var_name='Dataset', value_name='R2_Score')\n\npalette = ['#488286', '#B7D5D4'] \n\nsns.lineplot(data=df_long, x='Model', y='R2_Score', hue='Dataset', marker='o', palette=palette)\nplt.xticks(rotation=45, fontsize=12)\nplt.title('R¬≤ Score Comparison: Train vs Test for All Models')\nplt.xlabel('Model')\nplt.ylabel('R¬≤ Score')\nplt.legend(title='Dataset')\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n# Additional line plot for MAE comparison\nplt.figure(figsize=(14, 6))\nplt.plot(results_df['Model'], results_df['MAE_Train'], marker='o', label='Train MAE', color='#B7D5D4')\nplt.plot(results_df['Model'], results_df['MAE_Test'], marker='o', label='Test MAE', color='#305252')\nplt.xticks(rotation=45, ha='right')\nplt.title('MAE Comparison Across Models')\nplt.xlabel('Models')\nplt.ylabel('Mean Absolute Error')\nplt.legend()\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T19:06:05.119417Z","iopub.execute_input":"2025-07-05T19:06:05.11976Z","iopub.status.idle":"2025-07-05T19:07:08.970067Z","shell.execute_reply.started":"2025-07-05T19:06:05.119727Z","shell.execute_reply":"2025-07-05T19:07:08.969061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<span style=\"font-weight:bold; font-size:1.3em;\">‚úÖ 1. Top Performing Models</span>\n\n| Model               | R¬≤ Test   | MAE Test | Overfitting Assessment                                              |\n| ------------------- | --------- | -------- | ------------------------------------------------------------------- |\n| ‚úÖ Random Forest     | **0.930** | 44.22    | ‚ùå Minimal overfitting; very close train-test R¬≤ values             |\n| ‚úÖ Gradient Boosting | **0.931** | 51.09    | ‚ùå Minimal overfitting; consistent generalization                   |\n| ‚úÖ XGBoost           | **0.928** | 44.21    | ‚ö†Ô∏è Slight overfitting; small drop from train to test                |\n| ‚úÖ CatBoost          | **0.925** | 49.26    | ‚ö†Ô∏è Slight overfitting; steady performance with elevated error rates |\n| ‚úÖ LightGBM          | **0.913** | 57.17    | ‚ö†Ô∏è Mild overfitting; relatively higher test error                   |\n| ‚úÖ AdaBoost          | **0.892** | 82.89    | ‚ö†Ô∏è Moderate overfitting; highest MAE among top models               |\n\n**Key Insights:**\n\n* **Random Forest** and **Gradient Boosting** show top test R¬≤ with RF having lower MAE, making it a strong overall choice.\n* **XGBoost** and **CatBoost** perform similarly with slightly more overfitting.\n* **LightGBM** performs well but might improve with further tuning.\n* **AdaBoost** lags behind in precision, showing moderate overfitting.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">‚ö†Ô∏è 2. Highly Overfitted Models</span>\n\n| Model                 | R¬≤ Train | R¬≤ Test | Overfitting Severity                                              |\n| --------------------- | -------- | ------- | ----------------------------------------------------------------- |\n| ‚ùå Decision Tree       | 0.983    | 0.890   | ‚úÖ Noticeable overfitting; large train-test performance gap       |\n| ‚ùå K-Nearest Neighbors | 1.000    | 0.735   | ‚úÖ Severe overfitting; perfect train fit, poor generalization     |\n\n**Summary:**\n\n* **Decision Tree** and **KNN** models severely overfit, excelling on train but failing on unseen data.\n* Pruning or regularization needed; ensemble alternatives recommended.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">‚õî 3. Underperforming Models</span>\n\n| Model                          | R¬≤ Test | MAE Test |\n| ------------------------------ | ------- | -------- |\n| Support Vector Regressor (SVR) | 0.488   | 112.42   |\n| Huber Regressor                | 0.504   | 111.50   |\n| Ridge, Lasso, ElasticNet       | ~0.69   | ~120     |\n| Bayesian Ridge                 | ~0.69   | ~120     |\n\n**Observations:**\n\n* Linear and simpler models like **SVR**, **Huber**, and Ridge-family regressors struggle with the dataset‚Äôs nonlinear complexity.\n* High errors and low R¬≤ make them less reliable for premium prediction.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">‚ö° 4. Special Case: AdaBoost</span>\n\n| Model    | R¬≤ Train | R¬≤ Test | MAE Test |\n| -------- | -------- | ------- | -------- |\n| AdaBoost | 0.859    | 0.892   | 82.89    |\n\n* AdaBoost shows moderate overfitting and comparatively weaker performance.\n* Could be used cautiously with fine tuning or in ensemble stacking.\n\n---\n\n<span style=\"font-weight:bold; font-size:1.3em;\">üí° Final Recommendations</span>\n\n**Top recommended models:**\n\n* `Random Forest`: Best balance of accuracy and generalization.\n* `Gradient Boosting`, `XGBoost`, `CatBoost`, `LightGBM`: Strong contenders, worth tuning.\n* `AdaBoost`: Backup option with attention to hyperparameters.\n\n**Models to avoid:**\n\n* `Decision Tree`, `K-Nearest Neighbors`: Prone to overfitting; poor test results.\n* `SVR`, `Huber`, and linear models: Insufficient modeling capacity for data complexity.","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\")\n\n# Select only the best performing models from your results\nbest_model_names = ['Random Forest', 'Gradient Boosting', 'XGBoost', 'CatBoost', 'LightGBM']\n\nbest_models = {}\n\nfor name in best_model_names:\n    model = model_candidates[name]\n    print(f\"Training and tuning {name}...\")\n    if name in param_grids:\n        grid_search = GridSearchCV(model, param_grids[name], scoring='neg_mean_squared_error', cv=5, n_jobs=-1, verbose=0)\n        grid_search.fit(X_train_scaled, y_train)\n        best_model = grid_search.best_estimator_\n    else:\n        best_model = model\n        best_model.fit(X_train_scaled, y_train)\n    \n    best_models[name] = best_model\n\n# Example input for prediction\ninput_data = {\n    'age': 28,\n    'gender': 1,  # 1 for male, 0 for female\n    'bmi': 33.0,\n    'children': 3,\n    'discount_eligibility': 0,\n    'northeast': 0,\n    'northwest': 0,\n    'southeast': 1,\n    'southwest': 0,\n}\n\n# Convert input to DataFrame\ninput_df = pd.DataFrame([input_data])\n\n# One-hot encode categorical columns (same as training)\ninput_df = pd.get_dummies(input_df, columns=['gender', 'discount_eligibility'], drop_first=True)\n\n# Add any missing columns to match training features\nexpected_cols = X.columns.tolist()\nfor col in expected_cols:\n    if col not in input_df.columns:\n        input_df[col] = 0\n\n# Reorder columns to match training set\ninput_df = input_df[expected_cols]\n\n# Scale input features\ninput_scaled = scaler.transform(input_df)\n\n# Predict using the best models\nfor model_name in best_model_names:\n    model = best_models[model_name]\n    pred = model.predict(input_scaled)\n    print(f'\\033[96m{model_name} prediction: {pred[0]:.2f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T19:10:05.614151Z","iopub.execute_input":"2025-07-05T19:10:05.615207Z","iopub.status.idle":"2025-07-05T19:10:57.10167Z","shell.execute_reply.started":"2025-07-05T19:10:05.615164Z","shell.execute_reply":"2025-07-05T19:10:57.100824Z"}},"outputs":[],"execution_count":null}]}